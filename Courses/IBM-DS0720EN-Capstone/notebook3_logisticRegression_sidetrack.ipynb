{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align:center\">Data Science and Machine Learning Capstone Project</h1>\n",
    "<img style=\"float:right\" src=\"https://prod-edxapp.edx-cdn.org/static/edx.org/images/logo.790c9a5340cb.png\">\n",
    "<p style=\"text-align:center\">IBM: DS0720EN</p>\n",
    "<p style=\"text-align:center\">Question 3 of 4</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [Problem Statement](#problem)\n",
    "2. [Question 3](#question)\n",
    "3. [Data Cleaning and Standardization](#wrangling)\n",
    "4. [Analyzing and Visualizing](#analysis)\n",
    "5. [Concluding Remarks](#conclusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"problem\"></a>\n",
    "# Problem Statement\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The people of New York use the 311 system to report complaints about the non-emergency problems to local authorities. Various agencies in New York are assigned these problems. The Department of Housing Preservation and Development of New York City is the agency that processes 311 complaints that are related to housing and buildings.\n",
    "\n",
    "In the last few years, the number of 311 complaints coming to the Department of Housing Preservation and Development has increased significantly. Although these complaints are not necessarily urgent, the large volume of complaints and the sudden increase is impacting the overall efficiency of operations of the agency.\n",
    "\n",
    "Therefore, the Department of Housing Preservation and Development has approached your organization to help them manage the large volume of 311 complaints they are receiving every year.\n",
    "\n",
    "The agency needs answers to several questions. The answers to those questions must be supported by data and analytics. These are their  questions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"question\"></a>\n",
    "# Question 3\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does the Complaint Type that you identified in response to Question 1 have an obvious relationship with any particular characteristic or characteristic of the Houses?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach\n",
    "Determine whether or not there are any correlations between the building characteristics of buildings that experienced HEAT/HOT WATER complaints (from Question 1) relative to the building characteristics of all buildings in the PLUTO house database."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "Separately from this notebook:\n",
    "\n",
    "The [New York 311](https://data.cityofnewyork.us/Social-Services/311-Service-Requests-from-2010-to-Present/erm2-nwe9) data was loaded by [SODA](https://data.cityofnewyork.us/resource/fhrw-4uyv.csv?$limit=100000000&Agency=HPD&$select=created_date,unique_key,complaint_type,incident_zip,incident_address,street_name,address_type,city,resolution_description,borough,latitude,longitude,closed_date,location_type,status) into a Pandas DataFrame then saved to a pickle file.\n",
    "\n",
    "The [New York PLUTO](https://data.cityofnewyork.us/City-Government/Primary-Land-Use-Tax-Lot-Output-PLUTO-/xuk2-nczf) data was downloaded.  The instructions at ( Course / 1. Project Challenge Details and Setup / Datasets Used in this Course / Datasets ) said \"Use only the part that is specific to the borough that you are interested in based on your analysis.\"  My answer for Question 2 suggested the borough with the biggest HEAT/HOT WATER problem was BRONX.  For that reason, only the BX_18v1.csv file was loaded into a Pandas DataFrame then saved to a pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import scipy.stats as stats\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import jaccard_score, classification_report, log_loss, confusion_matrix\n",
    "\n",
    "files_path = 'C:\\\\Users\\\\It_Co\\\\Documents\\\\DataScience\\\\Capstone\\\\' #local\n",
    "#files_path = './' #IBM Cloud / Watson Studio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "b311 = pd.read_pickle(files_path + 'ny311full.pkl')\n",
    "\n",
    "#file_columns = ['Address','BldgArea','BldgDepth','BuiltFAR','CommFAR','FacilFAR','Lot','LotArea','LotDepth','NumBldgs','NumFloors','OfficeArea','ResArea','ResidFAR','RetailArea','YearBuilt','YearAlter1','ZipCode', 'YCoord', 'XCoord']\n",
    "#df = pd.read_csv(files_path + 'BX_18v1.csv', usecols=file_columns)\n",
    "#df = pd.concat([df, pd.read_csv(files_path + 'BK_18v1.csv', usecols=file_columns)])\n",
    "#df = pd.concat([df, pd.read_csv(files_path + 'MN_18v1.csv', usecols=file_columns)])\n",
    "#df = pd.concat([df, pd.read_csv(files_path + 'QN_18v1.csv', usecols=file_columns)])\n",
    "#df = pd.concat([df, pd.read_csv(files_path + 'SI_18v1.csv', usecols=file_columns)])\n",
    "#df.to_pickle(files_path + 'q3.pkl')\n",
    "\n",
    "df = pd.read_pickle(files_path + 'q3.pkl')\n",
    "\n",
    "print(\"NY 311 shape %s\" % (b311.shape,))\n",
    "print(\"PLUTO shape %s\" % (df.shape,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"wrangling\"></a>\n",
    "# Data Cleaning and Standardization\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correct or remove observations with missing or malformed data.  The 311 and the PLUTO data sets will need to be \"joined\" together by the common \"address\" element, which means the addresses will need to be standardized to a consistent layout to allow the addresses to be compared consistently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NY 311"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove columns deemed unnecessary for this question.\n",
    "b311.drop(['created_date','street_name','address_type','resolution_description','closed_date','location_type','status','unique_key','latitude','longitude'], axis=1, inplace=True)\n",
    "#Only use the combined \"heating and hot water\" complaints determined from Question 1.\n",
    "b311['complaint_type'] = b311['complaint_type'].str.upper()\n",
    "b311.drop(b311[b311[\"complaint_type\"].isin([\"HEAT/HOT WATER\",\"HEATING\"])==False].index, axis=0, inplace=True)\n",
    "b311.reset_index(drop=True, inplace=True)\n",
    "#Adjust all relevant strings to uppercase so different casing won't appear as separate values.\n",
    "b311['incident_address'] = b311['incident_address'].str.upper()\n",
    "b311['city'] = b311['city'].str.upper()\n",
    "b311['borough'] = b311['borough'].str.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print some initial information for comparison during later steps.\n",
    "print(\"shape %s\" % str(b311.shape))\n",
    "print(\"--nulls below--\")\n",
    "print(b311.isnull().sum())\n",
    "print(\"--types below--\")\n",
    "print(b311.dtypes)\n",
    "b311.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardize Borough\n",
    "Leveraging findings found while standardizing during Question 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b311['borough'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Correct rows where borough was entered in the city column with \"UNSPECIFIED\" in the borough column.\n",
    "five_boroughs = [\"BROOKLYN\",\"BRONX\",\"MANHATTAN\",\"QUEENS\",\"STATEN ISLAND\"]\n",
    "which_rows_to_adjust = b311[(b311[\"borough\"]=='UNSPECIFIED')&b311[\"city\"].isin(five_boroughs)].index\n",
    "b311.loc[which_rows_to_adjust,'borough']=b311.loc[which_rows_to_adjust,'city']\n",
    "b311.loc[which_rows_to_adjust,'city']=np.nan\n",
    "#Drop a few rows of ambiguous data.\n",
    "b311.drop(b311[(b311[\"borough\"]=='MANHATTAN')&(b311[\"city\"]=='BRONX')].index, axis=0, inplace=True)\n",
    "b311.reset_index(drop=True, inplace=True)\n",
    "#Fill in UNSPECIFIED borough when city was entered as NEW YORK.\n",
    "which_rows_to_adjust = b311[(b311[\"borough\"]=='UNSPECIFIED')&(b311[\"city\"]=='NEW YORK')].index\n",
    "b311.loc[which_rows_to_adjust,'borough']=\"MANHATTAN\"\n",
    "b311.loc[which_rows_to_adjust,'city']=np.nan\n",
    "#Although the city for most of the \"NEW YORK\" ones are the only ones that technically got the \"city\" column valued correctly,\n",
    "#since every other row uses city as \"neighborhood\":  Standardize these.\n",
    "which_rows_to_adjust = b311[(b311[\"city\"]=='NEW YORK')].index\n",
    "b311.loc[which_rows_to_adjust,'city']=np.nan\n",
    "#Any still unspecified boroughs with a value in \"city\" are in the Queens borough.  The \"city\" is actually a \"neighborhood\".\n",
    "queens_neighborhoods = b311[(b311['borough']=='UNSPECIFIED')&(b311['city'].isnull()==False)]['city'].unique()\n",
    "#Standardize borough for Queens neighborhoods.\n",
    "which_rows_to_adjust = b311[(b311[\"borough\"]=='UNSPECIFIED')&b311[\"city\"].isin(queens_neighborhoods)].index\n",
    "b311.loc[which_rows_to_adjust,'borough']=\"QUEENS\"\n",
    "#Null the borough if it still shows up as unspecified borough as there is no other information from which to derive it.\n",
    "which_rows_to_adjust = b311[(b311[\"borough\"]=='UNSPECIFIED')&b311[\"city\"].isnull()].index\n",
    "b311.loc[which_rows_to_adjust,'borough']=np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b311['borough'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter NY 311 data by borough to only include BRONX\n",
    "The instructions at ( Course / 1. Project Challenge Details and Setup / Datasets Used in this Course / Datasets ) said \"Use only the part that is specific to the borough that you are interested in based on your analysis.\"  My answer for Question 2 suggested the borough with the biggest HEAT/HOT WATER problem was BRONX.  For that reason, I am only considering the BRONX data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b311.drop(b311[(b311[\"borough\"]!='BRONX')].index, axis=0, inplace=True)\n",
    "b311.reset_index(drop=True, inplace=True)\n",
    "b311['borough'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove unnecessary columns\n",
    "These were only necessary to standardize and then filter by borough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove columns no longer necessary\n",
    "b311.drop(['borough','city'], axis=1, inplace=True)\n",
    "print(b311.shape)\n",
    "print(b311.isnull().sum())\n",
    "b311.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop observations with missing address as there will be no way to tie them to any PLUTO data.\n",
    "b311.dropna(subset=['incident_address'], axis=0, inplace=True)\n",
    "b311.reset_index(drop=True, inplace=True)\n",
    "print(b311.isnull().sum())\n",
    "b311['incident_address'].value_counts().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BRONX PLUTO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"shape %s\" % str(df.shape))\n",
    "print(\"---isnull follows---\")\n",
    "print(df.isnull().sum())\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adjust relevant strings to uppercase so different casing won't appear as separate values.\n",
    "df['Address'] = df['Address'].str.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the observations with missing address as there will be no way to tie them to any 311 data.\n",
    "df.dropna(subset=['Address'], axis=0, inplace=True)\n",
    "df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardization of Addresses\n",
    "Leveraging standardization methods developed during question 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"BRONX 311 unique addresses: %s\" % b311['incident_address'].unique().size)\n",
    "print(\"BRONX PLUTO unique addresses: %s\" % df['Address'].unique().size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color:Red;\">Determine how much overlap.  Ideally all 29K BRONX 311 addresses will be represented in the PLUTO set.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def WhichAddressesNotInPluto(howManyTopToShow):\n",
    "    complaints = set(b311['incident_address'].unique())\n",
    "    pluto = set(df['Address'].unique())\n",
    "    #Determine which 311 addresses were not found in PLUTO to gain insight as to why.\n",
    "    differences = complaints.difference(pluto)\n",
    "    print(\"Records not in PLUTO: %s.  Percent: %s\" % (len(differences), \"{:.2%}\".format(len(differences) / len(complaints))))\n",
    "    print(\"---Top %i---\" % howManyTopToShow)\n",
    "    print(b311[b311['incident_address'].isin(differences)]['incident_address'].value_counts().head(howManyTopToShow))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WhichAddressesNotInPluto(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color:Red;\">Over 20 percent of addresses in the BRONX 311 data cannot be merged to the BRONX PLUTO data prior to standardization.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Borrow some python functions developed during question 2\n",
    "With minor improvements to better work with full addresses instead of just street names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some street values have multiple spaces in a row.\n",
    "import re\n",
    "def standardize_spaces(raw):\n",
    "    result = raw.strip() #Remove leading and trailing spaces.\n",
    "    result = re.sub(' +', ' ', result) #Squeeze multiple adjacent spaces into just one space.\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some streets have problematic characters.  For example:  ST. ANN'S AVENUE also exists without period or apostophe.\n",
    "problem_characters = ['.', '\\'']\n",
    "def replace_problem_characters(raw):\n",
    "    result = raw\n",
    "    for (character) in problem_characters:\n",
    "        result = result.replace(character,'')\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Some words are sometimes entered in a non-standard way or with typos need to be standardized.\n",
    "word_replacements = [(\"AVE\",\"AVENUE\"),(\"ST\",\"STREET\"),(\"RD\",\"ROAD\"),(\"FT\",\"FORT\"),(\"BX\",\"BRONX\"),(\"MT\",\"MOUNT\"),\n",
    "                     (\"NICHLAS\",\"NICHOLAS\"),(\"NICHALOS\",\"NICHOLAS\"),(\"EXPRE\",\"EXPRESSWAY\"),(\"HARACE\",\"HORACE\"),\n",
    "                     (\"NO\",\"NORTH\"),(\"AV\",\"AVENUE\"),(\"CRK\",\"CREEK\"),(\"FR\",\"FATHER\"),(\"JR\",\"JUNIOR\"),(\"GR\",\"GRAND\"),\n",
    "                     (\"CT\",\"COURT\"),\n",
    "                     (\"SR\",\"\"), # Service Road.  These are always near a similarly named street.  Lump together.\n",
    "                     (\"QN\",\"QUEENS\"),\n",
    "                     (\"ND\",\"\"), # A space between a number and ND such as EAST 52 ND STREET.  Note ST and RD can be street or road.\n",
    "                     (\"PO\",\"POND\"),(\"BO\",\"BOND\"),(\"GRA\",\"GRAND\"),(\"REV\",\"REVEREND\"),(\"CO-OP\",\"COOP\"),\n",
    "                     (\"GRANDCONCOURSE\", \"GRAND CONCOURSE\"),(\"CENTRL\", \"CENTRAL\"),(\"BLVD\",\"BOULEVARD\"),\n",
    "                     (\"FREDRICK\", \"FREDERICK\"),(\"DOUGLAS\", \"DOUGLASS\"),(\"MALCOM\", \"MALCOMN\"),\n",
    "                     (\"NORTHEN\", \"NORTHERN\"),(\"AVNEUE\",\"AVENUE\"),\n",
    "                    (\"N\",\"NORTH\"),(\"S\",\"SOUTH\"),(\"E\",\"EAST\"),(\"W\",\"WEST\"),(\"SW\",\"SOUTHWEST\"),\n",
    "                             (\"NW\",\"NORTHWEST\"),(\"SE\",\"SOUTHEAST\"),(\"NE\",\"NORTHEAST\")]\n",
    "def replace_words(raw):\n",
    "    split_raw = raw.split()\n",
    "    for (old, new) in word_replacements:\n",
    "        found_at_index = next((i for i, x in enumerate(split_raw) if x==old), None)\n",
    "        if found_at_index!=None:\n",
    "            split_raw[found_at_index] = new\n",
    "    return standardize_spaces(\" \".join(split_raw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Some words are actually prefixes of the following word.  Example the LA prefix of LA GRANGE.\n",
    "word_prefixes = [\"DE\",\"MC\",\"LA\",\"VAN\",\"MAC\",\"CO\"]\n",
    "def concatenate_prefixes(raw):\n",
    "    split_raw = raw.split()\n",
    "    last_word = len(split_raw) - 1\n",
    "    for (prefix) in word_prefixes:\n",
    "        found_at_index = next((i for i, x in enumerate(split_raw) if x==prefix), None)\n",
    "        if found_at_index!=None:\n",
    "            if len(split_raw)>1:\n",
    "                if found_at_index != last_word:\n",
    "                    split_raw[found_at_index] = ''\n",
    "                    split_raw[found_at_index+1] = prefix + split_raw[found_at_index+1]\n",
    "                    return standardize_spaces(\" \".join(split_raw))\n",
    "    return raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Some phrases need custom replacement because they involve multiple words or easily mis-interpretted out of context.\n",
    "phrase_replacements = [(\"DR M L KING JR\",\"MARTIN LUTHER KING\"),(\"DR MARTIN L KING\",\"MARTIN LUTHER KING\"),\n",
    "    (\"MARTIN LUTHER KING\",\"MARTIN LUTHER KING\"),(\"MARTIN L KING JR\",\"MARTIN LUTHER KING\"),\n",
    "    (\"MARTIN L KING\",\"MARTIN LUTHER KING\"),(\"ST NICHOLAS\",\"SAINT NICHOLAS\"),(\"ST JOHN\",\"SAINT JOHN\"),\n",
    "    (\"ST MARK\",\"SAINT MARK\"),(\"ST ANN\",\"SAINT ANN\"),(\"ST LAWRENCE\",\"SAINT LAWRENCE\"),(\"ST PAUL\",\"SAINT PAUL\"),\n",
    "    (\"ST PETER\",\"SAINT PETER\"),(\"ST RAYMOND\",\"SAINT RAYMOND\"),(\"ST THERESA\",\"SAINT THERESA\"),(\"ST FELIX\",\"SAINT FELIX\"),\n",
    "    (\"ST MARY\",\"SAINT MARY\"),(\"ST OUEN\",\"SAINT OUEN\"),(\"ST JAMES\",\"SAINT JAMES\"),(\"ST GEORGE\",\"SAINT GEORGE\"),\n",
    "    (\"ST EDWARD\",\"SAINT EDWARD\"),(\"ST CHARLES\",\"SAINT CHARLES\"),(\"ST FRANCIS\",\"SAINT FRANCIS\"),\n",
    "    (\"ST ANDREW\",\"SAINT ANDREW\"),(\"ST JUDE\",\"SAINT JUDE\"),(\"ST LUKE\",\"SAINT LUKE\"),(\"ST JOSEPH\",\"SAINT JOSEPH\"),\n",
    "    (\"N D PERLMAN\",\"NATHAN PERLMAN\"),(\"O BRIEN\",\"OBRIEN\"),(\"F D R\",\"FDR\"),(\"EXPRESSWAY N SR\",\"EXPRESSWAY SR N\"),\n",
    "    (\"HOR HARDING\",\"HORACE HARDING\"),\n",
    "    (\"SERVICE ROAD\",\"\"), # These are always near a similarly named street. Lump together.\n",
    "    (\"DUMMY\",\"\"),(\"ADAM C POWELL\",\"ADAM CLAYTON POWELL\"),(\"POWELL COVE\",\"POWELLS COVE\")]\n",
    "def replace_phrases(raw):\n",
    "    result = raw\n",
    "    for (old,new) in phrase_replacements:\n",
    "        result = standardize_spaces(result.replace(old,new))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1ST, 2ND, 3RD, 4TH, ... nTH\n",
    "# Remove the suffixes leaving the numbers by themselves.\n",
    "number_suffixes = [\"ST\",\"ND\",\"RD\",\"TH\"]\n",
    "digits=[\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"0\"]\n",
    "def remove_number_suffixes(raw):\n",
    "    split_raw = raw.split()\n",
    "    for suffix in number_suffixes:\n",
    "        found_at_index = next((i for i, x in enumerate(split_raw) if x[0] in digits and x.endswith(suffix)), None) \n",
    "        if found_at_index!=None:            \n",
    "            split_raw[found_at_index] = split_raw[found_at_index][:-2]\n",
    "            return standardize_spaces(\" \".join(split_raw))\n",
    "    return raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_street(street):\n",
    "    r = street \n",
    "    r = standardize_spaces(r) \n",
    "    r = replace_problem_characters(r) \n",
    "    r = replace_phrases(r) \n",
    "    r = replace_words(r) \n",
    "    r = concatenate_prefixes(r) \n",
    "    r = remove_number_suffixes(r) \n",
    "    return r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardize the address in both the 311 and PLUTO data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b311['incident_address'] = b311['incident_address'].apply(standardize_street)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Address'] = df['Address'].apply(standardize_street)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See if there was an improvement in how well the 311 data can be merged with the PLUTO data by address.\n",
    "WhichAddressesNotInPluto(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color:Red;\">The percentage of addresses in the 311 data that can be matched to an entry in the PLUTO data set improved measurably.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the file to more quickly repeat additional work below.\n",
    "df.to_pickle(files_path + 'q3clean.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add latent variable to indicate if the address had any HEAT complaints.\n",
    "So that we can more conveniently see if it correlates to anything else in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle(files_path + 'q3clean.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Latent variable that indicates that there has been at least one complaint at the address.\n",
    "df['Complaints'] = df['Address'].isin(b311['incident_address'].unique()).astype('int64')\n",
    "grouped_addresses = b311.groupby('incident_address')\n",
    "grouped_addresses.groups\n",
    "\n",
    "#How many complaints at the address?\n",
    "#counts = b311['incident_address'].value_counts(sort=False).sort_index()\n",
    "#df['Complaints'] = df['Address'].map(counts)\n",
    "#df['Complaints'].replace(to_replace=np.nan, value=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#del b311 # Maybe save a little memory later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (df.shape)\n",
    "df['Complaints'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert each unique address into a unique number.\n",
    "#A category with a lot of different values, not a continuous value.\n",
    "address_lookup = {k:v + 1 for v, k in enumerate(df['Address'].unique().tolist())}\n",
    "df[\"Address\"] = df[\"Address\"].map(address_lookup)\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the file to more quickly repeat additional work below.\n",
    "df.to_pickle(files_path + 'q3clean.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color:Red;\">Saving the data allows restarting from this point without needing to wait for standardization of addresses and other time to be repeated as work continues.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imputation\n",
    "Handling \"missing\" features.  That are null or have a zero placeholder instead of a \"real\" value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle(files_path + 'q3clean.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_nulls():\n",
    "    for col in df.columns:\n",
    "        nulls = len(df[df[col].isnull()])\n",
    "        if nulls > 0:\n",
    "            print(\"%s:  %i of %i (%s)\" % (col, nulls, df.shape[0], \"{:.2%}\".format(nulls / df.shape[0])))\n",
    "check_nulls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replace with zeroes to kick these forward to the next section.\n",
    "df['ZipCode'].replace(to_replace=np.nan, value=0, inplace=True)\n",
    "df['XCoord'].replace(to_replace=np.nan, value=0, inplace=True)\n",
    "df['YCoord'].replace(to_replace=np.nan, value=0, inplace=True)\n",
    "check_nulls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_zeroes():\n",
    "    for col in df.columns:\n",
    "        zeroes = len(df[df[col].eq(0)])\n",
    "        if zeroes > 0:\n",
    "            print(\"%s:  %i of %i (%s)\" % (col, zeroes, df.shape[0], \"{:.2%}\".format(zeroes / df.shape[0])))\n",
    "check_zeroes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows that have only a small number of zero values in a column.\n",
    "df.drop(df[df[\"ZipCode\"].eq(0)].index, axis=0, inplace=True)\n",
    "df.drop(df[df[\"LotArea\"].eq(0)].index, axis=0, inplace=True)\n",
    "df.drop(df[df[\"LotDepth\"].eq(0)].index, axis=0, inplace=True)\n",
    "df.drop(df[df[\"BldgDepth\"].eq(0)].index, axis=0, inplace=True)\n",
    "df.drop(df[df[\"BuiltFAR\"].eq(0)].index, axis=0, inplace=True)\n",
    "df.drop(df[df[\"FacilFAR\"].eq(0)].index, axis=0, inplace=True)\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "check_zeroes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# May need to drop more rows, that had columns with a lot of zeroes in multiple columns.\n",
    "df.drop(df[df[\"NumBldgs\"].eq(0)].index, axis=0, inplace=True)\n",
    "df.drop(df[df[\"NumFloors\"].eq(0)].index, axis=0, inplace=True)\n",
    "df.drop(df[df[\"YearBuilt\"].eq(0)].index, axis=0, inplace=True)\n",
    "df.drop(df[df[\"XCoord\"].eq(0)].index, axis=0, inplace=True)\n",
    "df.drop(df[df[\"YCoord\"].eq(0)].index, axis=0, inplace=True)\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "check_zeroes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color:Red;\">Several features have a significant number of zeroes.  Some even a vast majority of zero values.  This means simply the fact that there was a zero or not could be an important thing to correlate.  I am going to create latent variables for each simply to record the presence of the value or not.  These latent variables can themselves be used to filter when doing regression checks on the root feature.  The latent variables themselves can even be the subject of regression checks.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.insert(len(df.columns) - 1, 'IsResArea', df['ResArea'].ne(0).astype('int64'))\n",
    "df.insert(len(df.columns) - 1, 'IsOfficeArea', df['OfficeArea'].ne(0).astype('int64'))\n",
    "df.insert(len(df.columns) - 1, 'IsRetailArea', df['RetailArea'].ne(0).astype('int64'))\n",
    "df.insert(len(df.columns) - 1, 'IsYearAlter1', df['YearAlter1'].ne(0).astype('int64'))\n",
    "df.insert(len(df.columns) - 1, 'IsResidFAR', df['ResidFAR'].ne(0).astype('int64'))\n",
    "df.insert(len(df.columns) - 1, 'IsCommFAR', df['CommFAR'].ne(0).astype('int64'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['IsResArea'].value_counts())\n",
    "print(df['IsOfficeArea'].value_counts())\n",
    "print(df['IsRetailArea'].value_counts())\n",
    "print(df['IsYearAlter1'].value_counts())\n",
    "print(df['IsResidFAR'].value_counts())\n",
    "print(df['IsCommFAR'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling, Centering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_and_center(column_name):\n",
    "    scale=MinMaxScaler()\n",
    "    scale.fit(df[[column_name]])\n",
    "    df[[column_name]]=scale.transform(df[[column_name]])\n",
    "scale_and_center('LotArea')\n",
    "scale_and_center('BldgArea')\n",
    "scale_and_center('NumBldgs')\n",
    "scale_and_center('NumFloors')\n",
    "scale_and_center('LotDepth')\n",
    "scale_and_center('BldgDepth')\n",
    "scale_and_center('YearBuilt')\n",
    "scale_and_center('BuiltFAR')\n",
    "scale_and_center('FacilFAR')\n",
    "scale_and_center('XCoord')\n",
    "scale_and_center('YCoord')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_and_center_latent(column_name, latent):\n",
    "    scale=MinMaxScaler()\n",
    "    #scale.fit(df.loc[df[latent].eq(1), column_name].to_frame())\n",
    "    df.loc[df[latent].eq(1), column_name] = scale.fit_transform(df.loc[df[latent].eq(1), column_name].to_frame())\n",
    "scale_and_center_latent('ResArea', 'IsResArea')\n",
    "scale_and_center_latent('OfficeArea', 'IsOfficeArea')\n",
    "scale_and_center_latent('RetailArea', 'IsRetailArea')\n",
    "scale_and_center_latent('YearAlter1', 'IsYearAlter1')\n",
    "scale_and_center_latent('ResidFAR', 'IsResidFAR')\n",
    "scale_and_center_latent('CommFAR', 'IsCommFAR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the cleaned data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_pickle(files_path + 'q3clean.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"analysis\"></a>\n",
    "# Analyzing and Visualizing\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load combined and cleaned data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle(files_path + 'q3clean.pkl')\n",
    "print(df.shape[0])\n",
    "print(len(df[df['Complaints'].ne(0)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze, using visualizations as necessary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression\n",
    "Logistic Regression instead of Linear chosen because the Y values are zero or one, so this is a classifier rather than continuous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regress_logistically(d):\n",
    "    print(\"-----\")\n",
    "    print(d.columns)\n",
    "    y_data = d['Complaints']\n",
    "    x_data = d.drop('Complaints',axis=1)\n",
    "    x_train,x_test,y_train,y_test = train_test_split(x_data,y_data,test_size=0.20,random_state=1)\n",
    "    print(\"number of test samples: \", x_test.shape[0])\n",
    "    print(\"number of training samples: \",x_train.shape[0])\n",
    "    m = linear_model.LogisticRegression(solver='lbfgs', multi_class='ovr', max_iter=250)\n",
    "    m.fit(x_train, y_train)\n",
    "    y_hat = m.predict(x_test)\n",
    "    print(\"Jaccard score: \", jaccard_score(y_test, y_hat))\n",
    "    print(\"Log loss: \", log_loss(y_test, m.predict_proba(x_test)))\n",
    "    print(confusion_matrix(y_test, y_hat, labels=[1,0]))\n",
    "    print(classification_report(y_test, y_hat))\n",
    "    coefficients = []\n",
    "    for i in range(m.coef_.size):\n",
    "        coefficients.append((d.columns[i], m.coef_[0,i]))\n",
    "    def sortco(a):\n",
    "        return abs(a[1])\n",
    "    coefficients.sort(key=sortco)\n",
    "    print(coefficients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#See how the conditional items fare.\n",
    "regress_logistically(df[df['IsResArea'].eq(1)][['ResArea','Complaints']])\n",
    "regress_logistically(df[df['IsOfficeArea'].eq(1)][['OfficeArea','Complaints']])\n",
    "regress_logistically(df[df['IsRetailArea'].eq(1)][['RetailArea','Complaints']])\n",
    "regress_logistically(df[df['IsYearAlter1'].eq(1)][['YearAlter1','Complaints']])\n",
    "regress_logistically(df[df['IsResidFAR'].eq(1)][['ResidFAR','Complaints']])\n",
    "regress_logistically(df[df['IsCommFAR'].eq(1)][['CommFAR','Complaints']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See how all the other items fare.\n",
    "regress_logistically(df[['LotArea','BldgArea', 'NumBldgs','NumFloors','LotDepth','BldgDepth','YearBuilt',\n",
    "                          'BuiltFAR','FacilFAR','XCoord','YCoord','Complaints']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take out the location based items and dubious items.\n",
    "regress_logistically(df[['LotArea','BldgArea', 'NumBldgs','NumFloors','LotDepth','BldgDepth','YearBuilt',\n",
    "                          'Complaints']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color:Red;\">The relationships are too weak to get a good scoring logistic regression.  The fact that about 85% of the properties did not have a complaint means that even a coin-toss model has high F1 score when it predicts \"tails\", but very poor scoring when predicting \"heads\".  The top row of the confusion matrix is always very poor.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pearson Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for correlations with heatmap.\n",
    "AllForHeat = df.corr()\n",
    "#AllForHeat.shape\n",
    "AllForHeat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "im = ax.pcolor(AllForHeat, cmap='RdBu_r')\n",
    "row_labels = df.columns; col_labels = df.columns\n",
    "ax.set_xticklabels(row_labels, minor = False);ax.set_yticklabels(col_labels, minor = False)\n",
    "#move ticks and labels to the center.\n",
    "ax.set_xticks(np.arange(AllForHeat.shape[1]) + 0.5, minor=False)\n",
    "ax.set_yticks(np.arange(AllForHeat.shape[0]) + 0.5, minor=False)\n",
    "ax.set_title(\"heat map of correlations\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.colorbar(im)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color:Red;\">Looking along the top row or rightmost column, there appear to be some correlations, though none super strong.  Looking at the actual numbers might help see them better.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#See numerically what the strongest correlations are.\n",
    "AllForHeat[(AllForHeat['Complaints'].ge(0.05))|(AllForHeat['Complaints'].le(-0.05))]['Complaints'].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the pearson correlations and confidence measures\n",
    "for col in df.columns:\n",
    "    pearson_coef, p_value = stats.pearsonr(df[col], df['Complaints'])\n",
    "    if abs(pearson_coef) > 0.20 and p_value < 0.10:\n",
    "        print(col, pearson_coef, p_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color:Red;\">Not really any \"strong\" correlations, but some weak ones with high confidence.  Double check the details of a few of these.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Address correlates?  Why?\n",
    "total = df.shape[0]\n",
    "complainers = df[df['Complaints'].eq(1)].shape[0]\n",
    "print (total, complainers, complainers / total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color:Red;\">I think address correlates just because each address either had a complaint or not, so the correlation is about the same as the percentage that had a complaint.  Also, question 2 already narrowed it down geographically so I'm going to dismiss this \"correlation\" as not relevant to the question at hand.  To a lesser extent the X Coordinate and ZIP code are in the same boat, though there may be a higher (but still very low) correlation than there is with address.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#YearAlter1 correlates?  Why?\n",
    "total = df.shape[0]\n",
    "never_altered = df[df['Complaints'].eq(0)].shape[0]\n",
    "print (total, never_altered, never_altered / total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Alteration year - take a closer look.\n",
    "alterations = df[['YearAlter1','Complaints']]\n",
    "alterations = pd.concat([alterations, pd.get_dummies(alterations['YearAlter1'])], axis=1)\n",
    "for col in alterations.columns:\n",
    "    pearson_coef, p_value = stats.pearsonr(alterations[col], alterations['Complaints'])\n",
    "    if abs(pearson_coef) > 0.05 and p_value < 0.10:\n",
    "        print(col, pearson_coef, p_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color:Red;\">The only even mildly significant YearAlter1 correlations are between the observations where this field is zero, which is about 2/3 of the data.  My conclusion is that this is a factor of the high number of zeroes, causing the correlation to skew toward lower numbers, hence the lowest numbers, the zeroes themselves, are considered more correlated.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check a few of these to see if they are different data points that capture the same relationship.\n",
    "def see_pearson(a,b):\n",
    "    pearson_coef, p_value = stats.pearsonr(df[a], df[b])\n",
    "    print(a, b, pearson_coef, p_value)\n",
    "see_pearson('ResidFAR','FacilFAR')\n",
    "see_pearson('ResidFAR','BuiltFAR')\n",
    "see_pearson('FacilFAR','BuiltFAR')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color:Red;\">ResidFAR and FacilFAR are strongly correlated to each other.  The documentation says FAR is the \"Floor Area Ratio\" between the building and the LOT.  The Resid and Facility variations are about the same.  What they have in common is they are both \"not commercial\".</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize strongest Pearson correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Visualize\n",
    "#df.corr()['Complaints'].map(abs).plot(kind='bar', title='Correlation with heating complaints',figsize=(8,3))\n",
    "df.corr()['Complaints'].plot(kind='bar', grid=True, title='Correlation with heating complaints',figsize=(8,3))\n",
    "plt.xlabel('Building Characteristics'); plt.ylabel('Correlation')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"conclusion\"></a>\n",
    "# Concluding Remarks\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The HEATING/HOT WATER complaint types (Question 1) reported in the BRONX borough (question 2) have an obvious relationship with the following housing characteristics:\n",
    "<ul>\n",
    "<li>NumFloors (Number of Floors):  0.37 correlation.\n",
    "<li>BuiltFAR (Total building floor area divided by the area of the tax lot):  0.35 correlation.\n",
    "<li>ResidFAR ():  0.32 correlation.\n",
    "</ul>\n",
    "The inability to compute an effective logistic regression is further evidence that the relationships are not especially strong, but the pearson correlation confidence numbers suggest these measurements are accurate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So basically the taller and bigger the floor area of the building, the more heating complaints from that building.  Especially residential buildings."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
